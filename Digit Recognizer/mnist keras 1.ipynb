{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv(f\"C:\\\\Users\\\\Arun\\\\Desktop\\\\kaggle\\\\digit-recognizer\\\\train.csv\")\n",
    "test_data = pd.read_csv(f\"C:\\\\Users\\\\Arun\\\\Desktop\\\\kaggle\\\\digit-recognizer\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (42000, 785)\n",
      "Test (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "print('Train '+ str(train_data.shape))\n",
    "print('Test '+ str(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1790fa20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBpP4fHWIC84nJ3xsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x172d1908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_data = train_data['label']\n",
    "#y_data = pd.get_dummies(y_data)\n",
    "X_data = train_data.drop('label',axis=1)\n",
    "del train_data\n",
    "plt.imshow(X_data.iloc[3].values.reshape(28,28), cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize\n",
    "X_data = X_data / 255.0\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "# Reshape\n",
    "X_data = X_data.values.reshape(-1,28,28,1)\n",
    "test_data = test_data.values.reshape(-1,28,28,1)\n",
    "\n",
    "# convert to one-hot-encoding\n",
    "y_data = to_categorical(y_data, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training and validation set\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data, y_data, test_size = 0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DigitRecognizerModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the DigitRecognizerModel.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # input placeholder with shape input_shape\n",
    "    X_input = Input((28,28,1))\n",
    "    \n",
    "    # layer 1\n",
    "    X = Conv2D(8, (5, 5), strides = (1, 1), name = 'conv0', padding='same', activation='relu')(X_input)\n",
    "    X = MaxPooling2D(pool_size=(2,2),name='maxpool')(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    \n",
    "    # layer 2\n",
    "    X = Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', activation ='relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2,2), strides=(2,2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    \n",
    "    # layer 3\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(256, activation = \"relu\")(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    \n",
    "    # output layer\n",
    "    X = Dense(10, activation = \"softmax\")(X)\n",
    "    \n",
    "    # model\n",
    "    model = Model(inputs = X_input, outputs = X, name='DigitRecognizerModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = DigitRecognizerModel(X_train.shape[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "37800/37800 [==============================] - 44s 1ms/step - loss: 0.0471 - acc: 0.9833\n",
      "Epoch 2/20\n",
      "37800/37800 [==============================] - 45s 1ms/step - loss: 0.0215 - acc: 0.9924\n",
      "Epoch 3/20\n",
      "37800/37800 [==============================] - 44s 1ms/step - loss: 0.0176 - acc: 0.9938\n",
      "Epoch 4/20\n",
      "37800/37800 [==============================] - 45s 1ms/step - loss: 0.0151 - acc: 0.9948\n",
      "Epoch 5/20\n",
      "37800/37800 [==============================] - 44s 1ms/step - loss: 0.0133 - acc: 0.9955\n",
      "Epoch 6/20\n",
      "37800/37800 [==============================] - 41s 1ms/step - loss: 0.0126 - acc: 0.9957\n",
      "Epoch 7/20\n",
      "37800/37800 [==============================] - 43s 1ms/step - loss: 0.0111 - acc: 0.9963\n",
      "Epoch 8/20\n",
      "37800/37800 [==============================] - 43s 1ms/step - loss: 0.0108 - acc: 0.9963\n",
      "Epoch 9/20\n",
      "37800/37800 [==============================] - 43s 1ms/step - loss: 0.0103 - acc: 0.9965\n",
      "Epoch 10/20\n",
      "37800/37800 [==============================] - 44s 1ms/step - loss: 0.0102 - acc: 0.9966\n",
      "Epoch 11/20\n",
      "37800/37800 [==============================] - 44s 1ms/step - loss: 0.0089 - acc: 0.9968\n",
      "Epoch 12/20\n",
      "37800/37800 [==============================] - 44s 1ms/step - loss: 0.0084 - acc: 0.9971\n",
      "Epoch 13/20\n",
      "37800/37800 [==============================] - 45s 1ms/step - loss: 0.0083 - acc: 0.9970\n",
      "Epoch 14/20\n",
      "37800/37800 [==============================] - 45s 1ms/step - loss: 0.0084 - acc: 0.9972\n",
      "Epoch 15/20\n",
      "37800/37800 [==============================] - 45s 1ms/step - loss: 0.0077 - acc: 0.9975\n",
      "Epoch 16/20\n",
      "37800/37800 [==============================] - 45s 1ms/step - loss: 0.0075 - acc: 0.9976\n",
      "Epoch 17/20\n",
      "37800/37800 [==============================] - 45s 1ms/step - loss: 0.0077 - acc: 0.9974\n",
      "Epoch 18/20\n",
      "37800/37800 [==============================] - 44s 1ms/step - loss: 0.0078 - acc: 0.9975\n",
      "Epoch 19/20\n",
      "37800/37800 [==============================] - 45s 1ms/step - loss: 0.0070 - acc: 0.9976\n",
      "Epoch 20/20\n",
      "37800/37800 [==============================] - 45s 1ms/step - loss: 0.0072 - acc: 0.9976: 1s - lo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15f9f5c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=Y_train,batch_size=16,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200/4200 [==============================] - 2s 453us/step\n",
      "Loss = 0.008654725619784412\n",
      "Test Accuracy = 0.9974285663877215\n"
     ]
    }
   ],
   "source": [
    "# evaluate model using validation data set\n",
    "preds = model.evaluate(x=X_val,y=Y_val)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.read_csv(f\"C:\\\\Users\\\\Arun\\\\Desktop\\\\kaggle\\\\digit-recognizer\\\\sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test data\n",
    "Y_pred = model.predict(test_data)\n",
    "output['Label'] = np.argmax(Y_pred,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapted from https://www.kaggle.com/kanncaa1/convolutional-neural-network-cnn-tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
